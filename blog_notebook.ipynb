{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "280c2486-cf28-4e32-9b85-3caf4296a3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/spacy3/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13eb78f4-b61c-4e56-a1e2-153fb0482523",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c7e591-7d7b-4d92-8f61-d5bb03af5dd7",
   "metadata": {},
   "source": [
    "## Bagging to BERT: Sentiment analysis three ways\n",
    "This notebook accompanies the ODSC blog post introducing the Bagging to BERT workshop.  This will be expanded for the full workshop.\n",
    "\n",
    "Up first is some preprocessing.  You'll either need to download the [imdb review data](https://ai.stanford.edu/~amaas/data/sentiment/) and save it to this directory OR download the processed data from [here](https://drive.google.com/file/d/1oN_fO91IBkDHD_u6WXiUCvhhyNexQDJq/view?usp=sharinghttps://drive.google.com/file/d/1oN_fO91IBkDHD_u6WXiUCvhhyNexQDJq/view?usp=sharing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9b3c4ac0-5a90-49ef-b03b-378932d13c9a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # processing the original data into DataFrame\n",
    "# # here for reference, don't need to run this if you're using reviews.pkl.gz\n",
    "# source_path = Path('./aclImdb/')\n",
    "# #neg_files = source_path.glob('./*/neg/*.txt')\n",
    "# #pos_files = source_path.glob('./*/pos/*.txt')\n",
    "# all_files = []\n",
    "# for f in source_path.glob('./*/*/*.txt'):\n",
    "#     filename = f.as_posix()\n",
    "#     if 'unsup' not in filename:\n",
    "#         # split up into useful components\n",
    "#         _, split, sent, idx = filename.split('/')\n",
    "#         idx = int(idx.split('_')[0])\n",
    "#         all_files.append([idx, split, sent, f.read_text()])\n",
    "# review_df = pd.DataFrame(all_files)\n",
    "# review_df.columns = ['idx', 'split', 'label', 'text']\n",
    "# # some minor html cruft is in here\n",
    "# review_df['text'] = review_df['text'].str.replace('<br /><br />', '')\n",
    "# review_df = review_df.to_pickle('reviews.pkl.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "e28f61ea-8e62-4d21-8f5a-6e3eb1bed2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can skip here if you already have reviews.pkl.gz\n",
    "review_df = pd.read_pickle('reviews.pkl.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db840c0f-9f6d-4058-9913-356eb700dcd0",
   "metadata": {},
   "source": [
    "### Word counts\n",
    "A very basic way to use a sanitized list of tokens is to do a word count. This unlocks a lot of insights right off and is an important step in exploratory data analysis in text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e2646a7d-b0f4-406d-bb68-7f5351b425b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n",
      " Alan Rickman & Emma Thompson give good performances with southern/New Orleans accents in this detective flick. It's worth seeing for their scenes- and Rickman's scene with Hal Holbrook. These three actors mannage to entertain us no matter what the movie, it seems. The plot for the movie shows potential, but one gets the impression in watching the film that it was not pulled off as well as it could have been. The fact that it is cluttered by a rather uninteresting subplot and mostly uninteresting kidnappers really muddles things. The movie is worth a view- if for nothing more than entertaining performances by Rickman, Thompson, and Holbrook. \n",
      "\n",
      "Positive\n",
      " Based on an actual story, John Boorman shows the struggle of an American doctor, whose husband and son were murdered and she was continually plagued with her loss. A holiday to Burma with her sister seemed like a good idea to get away from it all, but when her passport was stolen in Rangoon, she could not leave the country with her sister, and was forced to stay back until she could get I.D. papers from the American embassy. To fill in a day before she could fly out, she took a trip into the countryside with a tour guide. \"I tried finding something in those stone statues, but nothing stirred in me. I was stone myself.\" Suddenly all hell broke loose and she was caught in a political revolt. Just when it looked like she had escaped and safely boarded a train, she saw her tour guide get beaten and shot. In a split second she decided to jump from the moving train and try to rescue him, with no thought of herself. Continually her life was in danger. Here is a woman who demonstrated spontaneous, selfless charity, risking her life to save another. Patricia Arquette is beautiful, and not just to look at; she has a beautiful heart. This is an unforgettable story. \"We are taught that suffering is the one promise that life always keeps.\"\n"
     ]
    }
   ],
   "source": [
    "# take a positive and negative review for examples\n",
    "# we'll use Star Wars Episode VI since everyone likes a Star War\n",
    "neg_review = review_df.loc[(review_df.label=='neg')].iloc[0]['text']\n",
    "pos_review = review_df[(review_df.label=='pos')].iloc[0]['text']\n",
    "print('Negative\\n', neg_review, '\\n')\n",
    "print('Positive\\n', pos_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1ee60e2b-b1b5-4342-9e7e-8e0e3f17538f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'the': 4, 'it': 4, 'for': 3, 'and': 3, 'The': 3, 'performances': 2, 'with': 2, 'in': 2, 'worth': 2, 'Holbrook.': 2, 'movie': 2, 'that': 2, 'as': 2, 'is': 2, 'by': 2, 'a': 2, 'uninteresting': 2, 'Alan': 1, 'Rickman': 1, '&': 1, 'Emma': 1, 'Thompson': 1, 'give': 1, 'good': 1, 'southern/New': 1, 'Orleans': 1, 'accents': 1, 'this': 1, 'detective': 1, 'flick.': 1, \"It's\": 1, 'seeing': 1, 'their': 1, 'scenes-': 1, \"Rickman's\": 1, 'scene': 1, 'Hal': 1, 'These': 1, 'three': 1, 'actors': 1, 'mannage': 1, 'to': 1, 'entertain': 1, 'us': 1, 'no': 1, 'matter': 1, 'what': 1, 'movie,': 1, 'seems.': 1, 'plot': 1, 'shows': 1, 'potential,': 1, 'but': 1, 'one': 1, 'gets': 1, 'impression': 1, 'watching': 1, 'film': 1, 'was': 1, 'not': 1, 'pulled': 1, 'off': 1, 'well': 1, 'could': 1, 'have': 1, 'been.': 1, 'fact': 1, 'cluttered': 1, 'rather': 1, 'subplot': 1, 'mostly': 1, 'kidnappers': 1, 'really': 1, 'muddles': 1, 'things.': 1, 'view-': 1, 'if': 1, 'nothing': 1, 'more': 1, 'than': 1, 'entertaining': 1, 'Rickman,': 1, 'Thompson,': 1})\n"
     ]
    }
   ],
   "source": [
    "# base python word count - split on whitespace, use Counter object)\n",
    "print(Counter(neg_review.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112c7f82-7e21-42a1-9f70-0c97345ab36a",
   "metadata": {},
   "source": [
    "Already see some things that need to be considered; capitalization treats \"The\" and \"the\" differently, words like \"the\" and \"it\" dominate counts.\n",
    "\n",
    "Luckily, scikit-learn's CountVectorizer allows for simple preprocessing like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3868f067-62e5-4227-9e81-76485ec7cfdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x76 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 76 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scikit-learn's countvectorizer\n",
    "count = CountVectorizer()\n",
    "neg_vec = count.fit_transform([neg_review])\n",
    "neg_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b990345-6215-4e55-845e-53ad8d5c5e80",
   "metadata": {},
   "source": [
    "`CountVectorizer` outputs a sparse matrix by default.  We can convert that to a normal numpy array and stitch it together with the vocabulary from the `fit()` call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5628123c-2f0a-48b5-afa3-faf6284f60f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accents': 1, 'actors': 1, 'alan': 1, 'and': 3, 'as': 2, 'been': 1, 'but': 1, 'by': 2, 'cluttered': 1, 'could': 1, 'detective': 1, 'emma': 1, 'entertain': 1, 'entertaining': 1, 'fact': 1, 'film': 1, 'flick': 1, 'for': 3, 'gets': 1, 'give': 1, 'good': 1, 'hal': 1, 'have': 1, 'holbrook': 2, 'if': 1, 'impression': 1, 'in': 2, 'is': 2, 'it': 5, 'kidnappers': 1, 'mannage': 1, 'matter': 1, 'more': 1, 'mostly': 1, 'movie': 3, 'muddles': 1, 'new': 1, 'no': 1, 'not': 1, 'nothing': 1, 'off': 1, 'one': 1, 'orleans': 1, 'performances': 2, 'plot': 1, 'potential': 1, 'pulled': 1, 'rather': 1, 'really': 1, 'rickman': 3, 'scene': 1, 'scenes': 1, 'seeing': 1, 'seems': 1, 'shows': 1, 'southern': 1, 'subplot': 1, 'than': 1, 'that': 2, 'the': 7, 'their': 1, 'these': 1, 'things': 1, 'this': 1, 'thompson': 2, 'three': 1, 'to': 1, 'uninteresting': 2, 'us': 1, 'view': 1, 'was': 1, 'watching': 1, 'well': 1, 'what': 1, 'with': 2, 'worth': 2}\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    dict(zip(count.get_feature_names_out(), \n",
    "             neg_vec.toarray().flatten())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587c23b3-8d69-4a1b-b2e8-726f88eeb977",
   "metadata": {},
   "source": [
    "We can see the defaults have already done some amount of cleaning for us.\n",
    "\n",
    "### Deterministic Approach\n",
    "\n",
    "Let's try a deterministic approach, using word counts and a list of \"positive\" vs \"negative\" words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "251c4e13-03bf-4110-b15a-c128c12c7e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_words = [\"good\", \"great\", \"like\", \"loved\"]\n",
    "neg_words = [\"bad\", \"awful\", \"dislike\", \"hated\"]\n",
    "\n",
    "# we're going to use this train/test split throughout\n",
    "# we'll also use this seed for consistency\n",
    "# NOTE: Usually you'll want to do a separate validation set when choosing models/featuresets!\n",
    "seed = 37\n",
    "np.random.seed(seed)\n",
    "pct_train = 0.7\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    review_df['text'],\n",
    "    review_df['label'], train_size=pct_train)\n",
    "\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "train_vecs = cv.fit_transform(X_train)\n",
    "feats = cv.get_feature_names_out()\n",
    "pos_idxs = np.where(np.isin(feats, pos_words))[0]\n",
    "neg_idxs = np.where(np.isin(feats, neg_words))[0]\n",
    "train_det_score = train_vecs[:, pos_idxs].sum(1) - train_vecs[:, neg_idxs].sum(1)\n",
    "# easier for group-level score\n",
    "train_det_score = pd.Series(np.array(train_det_score).ravel(), \n",
    "                            index=X_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "765c8284-bb84-46e1-982e-928e140eeef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our threshold - the average score for negative, that or below = negative\n",
    "neg_thresh = train_det_score.groupby(review_df['label'].loc[X_train.index]).mean()['neg']\n",
    "test_vecs = cv.transform(X_test)\n",
    "test_det_score = test_vecs[:, pos_idxs].sum(1) - test_vecs[:, neg_idxs].sum(1)\n",
    "det_pred = test_det_score>neg_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b7ea3fdf-6cb4-4602-a384-dd05460ac21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.61      0.44      0.51      7522\n",
      "        True       0.56      0.71      0.63      7478\n",
      "\n",
      "    accuracy                           0.58     15000\n",
      "   macro avg       0.58      0.58      0.57     15000\n",
      "weighted avg       0.58      0.58      0.57     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    classification_report(y_pred=det_pred,\n",
    "                          y_true=y_test=='pos'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1f69c5-72b7-4ae9-8ed8-969818832c07",
   "metadata": {},
   "source": [
    "### Count Vector + Logistic Regression\n",
    "Here we try a count vector with Logistic Regression.  This alleviates the need for chosing an arbitrary set of terms and arbitrary threshold as above.\n",
    "\n",
    "Here I use scikit-learn's [Pipelines](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) functionality.  I won't try and explain that here, the docs do a much better job than I can.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "25a9d696-97ba-415d-84d2-503844edf3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = CountVectorizer(stop_words='english')\n",
    "\n",
    "count_pipeline = Pipeline(\n",
    "    steps=[(\"preprocessor\", count),\n",
    "          ('model', LogisticRegression(max_iter=500, solver='liblinear'))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "952acf7e-5a15-4bb8-88d1-258a634e5cce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8813333333333333"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "count_pipeline.fit(X_train, y_train)\n",
    "count_pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f58d9f7f-53b2-4cbb-92e6-2d5bac6dcf9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.89      0.88      0.88      7522\n",
      "         pos       0.88      0.89      0.88      7478\n",
      "\n",
      "    accuracy                           0.88     15000\n",
      "   macro avg       0.88      0.88      0.88     15000\n",
      "weighted avg       0.88      0.88      0.88     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    classification_report(y_pred=count_pipeline.predict(X_test),\n",
    "                          y_true=y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30256a1-982a-4aee-877c-7a7045580aa2",
   "metadata": {},
   "source": [
    "This is actually really good! 90% of the time we're predicting the right class with this model.  But can we do...better?\n",
    "\n",
    "### TF-IDF\n",
    "One thing we notice with count vectors is that all words are being counted the same.  We might want to use a weighting scheme to ensure that words that are more informative about the content are flagged as more important.  One weighting scheme is Term Frequency - Inverse Document Frequency (TF-IDF).\n",
    "\n",
    "Take as an example some kind of simplistic movie reviews.  We can already tell which words are most relevant to the specific content of each review (i.e. \"good\", \"bad\", \"great\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "385e0955-d159-4d27-8508-c1a5645a6dcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bad</th>\n",
       "      <th>good</th>\n",
       "      <th>great</th>\n",
       "      <th>movie</th>\n",
       "      <th>the</th>\n",
       "      <th>was</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bad  good  great  movie  the  was\n",
       "0    0     1      0      1    1    1\n",
       "1    1     0      0      1    1    1\n",
       "2    0     0      1      1    1    1"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = ['The movie was good',\n",
    "        'The movie was bad',\n",
    "        'The movie was great']\n",
    "\n",
    "cv = CountVectorizer()\n",
    "vecs = cv.fit_transform(docs).toarray()\n",
    "# we'll use pandas DF for easier display\n",
    "pd.DataFrame(vecs, columns=cv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9d93d3-aa56-4599-bd57-bd4e12232445",
   "metadata": {},
   "source": [
    "You'll notice that `vecs` contains the term frequencies.  If we use sklearn's `TfidfVectorizer`, it will calculate those term counts and then multiply them by the Inverse Document Frequency (IDF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "30f3611e-8860-411c-b297-e9d2eedca714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bad</th>\n",
       "      <th>good</th>\n",
       "      <th>great</th>\n",
       "      <th>movie</th>\n",
       "      <th>the</th>\n",
       "      <th>was</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.69903</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.412859</td>\n",
       "      <td>0.412859</td>\n",
       "      <td>0.412859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.69903</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.412859</td>\n",
       "      <td>0.412859</td>\n",
       "      <td>0.412859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.69903</td>\n",
       "      <td>0.412859</td>\n",
       "      <td>0.412859</td>\n",
       "      <td>0.412859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       bad     good    great     movie       the       was\n",
       "0  0.00000  0.69903  0.00000  0.412859  0.412859  0.412859\n",
       "1  0.69903  0.00000  0.00000  0.412859  0.412859  0.412859\n",
       "2  0.00000  0.00000  0.69903  0.412859  0.412859  0.412859"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "# we'll use pandas DF for easier display\n",
    "tfidf_vecs = tfidf.fit_transform(docs).toarray()\n",
    "tfidf_df = pd.DataFrame(tfidf_vecs, columns=tfidf.get_feature_names_out())\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a464164b-5882-48d7-8bb9-f48008ca8d92",
   "metadata": {},
   "source": [
    "You can see that the discriminative words have higher weight than the non-discriminative words.  \n",
    "\n",
    "It's worth noting here - in terms of \"separability\", having 0 v 1 (count of \"good\" vs count of \"bad\") might actually be better.  But these are highly curated examples - you can imagine cases where good and bad descriptive terms are mixed in a review, you want to capture the words that describe better the \"aboutness\" of the review.  (Think: \"This movie was not bad, it was good!\")\n",
    "\n",
    "Now let's fit our regression as above with TF-IDF vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "28f28bdf-371d-4dd7-a25b-63376f3f76d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use binary here to handle longer reviews\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "tfidf_pipeline = Pipeline(\n",
    "    steps=[(\"preprocessor\", tfidf),\n",
    "          ('model', LogisticRegression(max_iter=500, solver='liblinear'))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0c7f2b41-18ed-4fab-adee-d5b7bd44c90a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8912666666666667"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "tfidf_pipeline.fit(X_train, y_train)\n",
    "tfidf_pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "24b0c34d-00cb-4204-a538-8efc61755bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.90      0.88      0.89      7522\n",
      "         pos       0.88      0.90      0.89      7478\n",
      "\n",
      "    accuracy                           0.89     15000\n",
      "   macro avg       0.89      0.89      0.89     15000\n",
      "weighted avg       0.89      0.89      0.89     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    classification_report(y_pred=tfidf_pipeline.predict(X_test),\n",
    "                          y_true=y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "10b5b18b-9e54-4ba9-afd3-9a63318aa93e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>tfidf</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6408</th>\n",
       "      <td>awful</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6697</th>\n",
       "      <td>bad</td>\n",
       "      <td>2.0</td>\n",
       "      <td>316.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10324</th>\n",
       "      <td>boring</td>\n",
       "      <td>5.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22357</th>\n",
       "      <td>disappointing</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22360</th>\n",
       "      <td>disappointment</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27335</th>\n",
       "      <td>excellent</td>\n",
       "      <td>89233.0</td>\n",
       "      <td>89227.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28106</th>\n",
       "      <td>fails</td>\n",
       "      <td>17.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34078</th>\n",
       "      <td>great</td>\n",
       "      <td>89234.0</td>\n",
       "      <td>89112.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50143</th>\n",
       "      <td>mediocre</td>\n",
       "      <td>30.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51096</th>\n",
       "      <td>mildly</td>\n",
       "      <td>96.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52920</th>\n",
       "      <td>mst3k</td>\n",
       "      <td>59.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58910</th>\n",
       "      <td>perfect</td>\n",
       "      <td>89232.0</td>\n",
       "      <td>89234.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60835</th>\n",
       "      <td>poor</td>\n",
       "      <td>6.0</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78768</th>\n",
       "      <td>terrible</td>\n",
       "      <td>7.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86068</th>\n",
       "      <td>waste</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86083</th>\n",
       "      <td>wasting</td>\n",
       "      <td>100.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87842</th>\n",
       "      <td>worst</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 word    tfidf    count\n",
       "6408            awful      3.0      4.0\n",
       "6697              bad      2.0    316.0\n",
       "10324          boring      5.0     14.0\n",
       "22357   disappointing     16.0      5.0\n",
       "22360  disappointment     14.0      2.0\n",
       "27335       excellent  89233.0  89227.0\n",
       "28106           fails     17.0      9.0\n",
       "34078           great  89234.0  89112.0\n",
       "50143        mediocre     30.0      6.0\n",
       "51096          mildly     96.0     10.0\n",
       "52920           mst3k     59.0      7.0\n",
       "58910         perfect  89232.0  89234.0\n",
       "60835            poor      6.0     36.0\n",
       "78768        terrible      7.0     20.0\n",
       "86068           waste      4.0      1.0\n",
       "86083         wasting    100.0      8.0\n",
       "87842           worst      1.0      3.0"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking at the coefficients on the LR for each model\n",
    "word_feats = tfidf_pipeline['preprocessor'].get_feature_names_out()\n",
    "# get the largest by magnitude, stitch together to compare\n",
    "top = 10\n",
    "top_tfidf = np.argsort(np.abs(tfidf_pipeline['model'].coef_.flatten()))[-top:]\n",
    "top_count = np.argsort(np.abs(count_pipeline['model'].coef_.flatten()))[-top:]\n",
    "# top\n",
    "coef_df = pd.DataFrame([\n",
    "    word_feats,\n",
    "    tfidf_pipeline['model'].coef_.flatten(),\n",
    "    count_pipeline['model'].coef_.flatten()],\n",
    "    index=['word', 'tfidf', 'count']).T\n",
    "# normalize result for compare\n",
    "coef_df['tfidf'] = coef_df['tfidf'].rank()\n",
    "coef_df['count'] = coef_df['count'].rank()\n",
    "coef_df.loc[np.unique(np.concatenate([top_tfidf, top_count]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "a3e0d90b-ec5f-46ff-a3da-6684ad26778f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examples where there's disagreement\n",
    "tfidf_pred = tfidf_pipeline.predict_proba(X_test)[:, 1]\n",
    "count_pred = count_pipeline.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "c456f39f-b53c-4293-aadf-a4725db61123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# most interesting are where there's the largest disagreement\n",
    "top_disagree_idx = np.argsort(np.abs(tfidf_pred - count_pred))[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "895f8ac0-64a1-43c4-aeac-ba6488050b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assemble in df\n",
    "compare_df = pd.DataFrame([tfidf_pred, count_pred, y_test, X_test],\n",
    "            index=['tfidf_pred', 'count_pred', 'label', 'text']).T\n",
    "# would like some shorter mv reviews here\n",
    "compare_df['text'] = compare_df['text'].apply(lambda x: x[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "b91b04da-8631-4ce1-ae8c-3a6796572fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_df['tfidf_right'] = ((compare_df['tfidf_pred']>=0.5)&(compare_df['label']=='pos'))|\\\n",
    "    ((compare_df['tfidf_pred']<0.5)&(compare_df['label']=='neg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "7adecafb-b87b-4382-a781-c90d9d0c5e67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6621354380525597, 0.00953398674695978, 'pos',\n",
       "        \"Although recognized as the best film treatment of the difficulties of having a house in the country built (or bought) to your specifications, it is not the first, nor the last. In 1940 Jack Benny and Ann Sheridan were the leads in the film version of the comedy GEORGE WASHINGTON SLEPT HERE by George S. Kaufman and Moss Hart. And about fifteen years ago Shelly Long and Tom Hanks had the lead in THE MONEY PIT. The former was about moving into an 18th Century country house that...err, needs work. The latter was about building your dream house - in the late 1980s. Although the two films have their moments, both are not as good as BLANDINGS, which was based on an autobiographical novel of the same name.Jim Blandings and his wife Muriel (Cary Grant and Myrna Loy) are noticing the tight corners of their apartment, which they share with their two daughters Joan and Betsy (Sharyn Moffett and Connie Marshall). Although Blandings has a good income as an advertising executive (in 1948 he is making $15,000.00 a year, which was like making $90,000.00 today), and lives in a luxury apartment - which in the New York City of that day he rents! - he feels they should seek something better. He and Muriel take a drive into the country (Connecticut) and soon find an old ruin that both imagine can be fixed up as that dream house they want.And they both fall into the financial worm hole that buying land and construction can lead to. For one thing, they are so gung ho about the idea of building a home like this they fail to heed warning after warning by their wise, if cynical friend and lawyer Bill Cole (Melvin Douglas, in a nicely sardonic role). For example, Jim buys land from a Connecticut dealer (Ian Wolfe, sucking his chops quietly), with a check before double checking the correct cost for the land in that part of Connecticut. Bill points out he's paid about five or six thousand dollars more for the land than it is worth. There are problems about water supply that both Blandings just n\",\n",
       "        True],\n",
       "       [nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan],\n",
       "       [0.30912394320895137, 0.983967046134271, 'neg',\n",
       "        \"Some guy gets whacked. Right out in plain sight this other guy shoots him. He's got some bodyguards and they whack the killer, but a reporter gets interested. She goes to the hospital where they took the guy who got whacked. She walks in, and corners one bodyguard, but he doesn't feel like talking. I can't figure out why. It's not like anyone else is interested. She's the only reporter there. Anyway, her editor discourages her from working on this lame story. But hey, she does anyway. She goes to see the killer's sister & mom. A few minutes after she leaves they get whacked big time-- somebody blows up their trailer-- huge ball of fire. Then she searches out the bodyguard from the hospital. She finds him hungover on his boat, but a minute later they're both underwater sucking on a scuba tank 'cause three guys are trying to whack them (and have blown up the boat big time-- huge ball of fire). The reporter and the bodyguard whack two of the guys who are trying to whack them. In the course of the next hour another guy gets whacked crossing the street, there's a shootout with several stiffs in a warehouse, some car chases with wreckage & death, a fake suicide, etc. etc. Lotsa stiffs, all kindsa carnage. Great stuff, but what the reporter and the bodyguard can't figure is: why in hell the original guy got whacked. What's the motivation? Of course, it might help us to figure out why the reporter's even interested. Through almost all of this she's the only reporter on the story. Nobody else in the media cares. Not even with all the big fireballs and dead bodies. True, the original guy who got whacked wasn't exactly a celebrity. His job was a little bit dull. He was just the President. Yeah, the one who lives in the White House. Oh, and the bodyguard is a Secret Service agent.Is that the spoiler?It should be. After all there are no TV cameras, no other print reporters, no bloggers... just another one of those police blotter crimes...So what's the spoiler?Lemme think...No! W\",\n",
       "        True],\n",
       "       [nan, nan, nan, nan, nan],\n",
       "       [0.67985855662423, 2.030947383941691e-06, 'pos',\n",
       "        'Despite loving Rita Hayworth, finding the final few sequences of the film intriguing and being able to appreciate some of the subtler \"symbolic\" aspects of the cinematography, The Lady from Shanghai didn\\'t quite work for me. I had a problem with most of the performances, the script and the overall structure. And in a film that\\'s mostly people talking with each other in various situations, that\\'s quite a problem. The Lady from Shanghai ended up at a very low \"C\", or a 7, for me.The Lady from Shanghai is really all about Orson Welles\\' character, Michael O\\'Hara. O\\'Hara sees Elsa \"Rosalie\" Bannister (Rita Hayworth) in Central Park on a carriage ride and hits on her. Later, he saves her from a mugging and she takes a shine to him. O\\'Hara is a seaman from Ireland and the globetrotting Elsa happens to own a yacht with her husband, Arthur (Everett Sloane), a very famous and powerful California defense attorney. They talk O\\'Hara into working for them, despite his initial reservations--it seems to him, and to the audience, that Elsa is just looking for someone to have an affair with, and O\\'Hara doesn\\'t want to get involved.Shortly after going to work on their yacht, a strange man, George Grisby (Glenn Anders), who says he\\'s Arthur\\'s partner, shows up at a port of call and begins stirring up trouble. Eventually, Grisby asks O\\'Hara to enter into a very dubious and dangerous scheme. Foolishly, O\\'Hara agrees. Naturally it gets him into quite a bit of trouble, and eventually, a number of mysteries are revealed.Maybe my problems with the film lie in the fact that, so far, I\\'m not exactly a huge fan of Orson Welles, and here, he produces, writes, directs and consumes most of the screen time. I haven\\'t seen anywhere near the majority of Welles\\' work yet, but I\\'ve tended to like his later films better, when he became a bit more campy and performance-arty. I love F for Fake (Vérités et mensonges, 1974) for example, and I even kind of like his performance in Casino Royale (1967), when h',\n",
       "        True],\n",
       "       [0.747706707489527, 0.05706626167453301, 'pos',\n",
       "        \"This film was the first British teen movie to actually address the reality of the violent rock and roll society, rather than being a lucid parody of 1950s teenage life. In an attempt to celebrate the work of Liverpool's Junior Liaison Officers the opening title points out that 92% of potential delinquents, who have been dealt with under this scheme, have not committed a second crime. However, this becomes merely a pretext to the following teen-drama until the film's epilogue where we are instructed that we shouldn't feel responsible or sorry for such delinquents however mixed-up they might seem.Stanley Baker plays a tough detective who reluctantly takes on the post of Juvenile Liaison Officer. This hard-boiled character is a role typical of Baker. Having been currently on the trail of a notorious arsonist known as the firefly and does not relish the distraction of the transfer. However, as in all good police dramas he is led back full circle by a remarkable turn of events, back to his original investigation.His first case leads him to the home of two young children, Mary and Patrick Murphy (played by real-life brother and sister duo), who have committed a petty theft. Here he meets Cathie (satisfyingly portrayed by Anne Heywood) their older sister whom he eventually becomes romantically involved with. It quickly becomes obvious that the squalid environment of such inner-city estates is a breeding ground for juvenile delinquency.The elder brother of the Murphy family, Johnny, is the leader of a gang of rock and roll hoodlums. McCallum does an eye-catching turn as the Americanized mixed-up kid, who owes more to the likes of Marlon Brando, than any previous British star. One is reminded of Brando's character Johnny from 'The Wild One' who led a leather-clad gang of rebellious bikers in much the same way as this film's 'Johnny' leads his gang.Thankfully the preachiness of earlier Dearden crime dramas such as 'The Blue Lamp' is not so apparent. Instead we are presented w\",\n",
       "        True],\n",
       "       [0.24942190051091628, 0.9460945846170242, 'neg',\n",
       "        'i love bad shark movies. i really do. i laugh hysterically at them. and the scifi channel was having a marathon of them, culminating in the premier of their new original picture, hammerhead: shark frenzy. based on the previews, it looked like it was going to be HIGHLY amusing. essentially a remake of benchley\\'s creature, really. it was prefaced by a showing of shark attack 3: megalodon, which is shark movie hilarity at its best. i was in the mood; i was ready to go. bring it on, hammerhead-mad-scientist-man! oh, god, was that movie wrong.wrong, wrong, wrong.sick. twisted. MESSED UP.this is theoretical reproduction at its very worst, my friends. when a drugged-out girl is brought out of suspended animation and strapped to a table screaming her head off because the shark-human hybrid fetus the absolutely insane \"scientist\" deliberately implanted in her womb wants OUT... Jesus monkeys. that\\'s what i call disturbing.that\\'s really how the plot works: hmmm, thought the mad scientist. my son died of cancer, but i brought him back to life by combining his dna with that of a hammerhead shark, because sharks don\\'t succumb to cancer and further hammerheads reproduce via placenta. oh, look! a perfect amphibious being! i\\'ve created the next evolution of the human race! I KNOW! let\\'s make him reproduce! but darned if all those shark genes have\\'t made my son bloodthirsty; instead of raping the hot babes i keep sending into his little jungle paradise, he keeps eating him. but check this out! among the random people who have, by way of some unimportant plot twist, ended up on my research island, is the woman to whom my son was engaged before he died! i bet he\\'ll do HER! all this leads up to an extremely touching and heartfelt reunion: woman: you\\'re going to impregnate me? mad scientist: no. he is. (indicates thrashing shark-person in tank) how sweet.DO NOT WATCH THIS MOVIE. ever.',\n",
       "        True],\n",
       "       [nan, nan, nan, nan, nan],\n",
       "       [0.8663206305412761, 0.09600969317180277, 'pos',\n",
       "        \"Being the Beatlemaniac that I am, I approached Two Of Us with a combination of fear and fascination. Having seen 'In His Life: The John Lennon Story', I was quite concerned that Two Of Us will turn out no better. The fact that Aidan Quinn and Jared Harris look absolutely nothing like John Lennon and Paul McCartney \\x96 even with some make-up and proper hairdos \\x96 didn't help one bit.But I was more than a bit pleasantly surprised. It's probably thanks to the involvement of Michael Lindsay-Hogg, who directed Let It Be in 1970 and consequently probably knew John and Paul quite well, that the characters and the dialogue came across as convincing as they did. (The writing credit for Two Of Us is given to a man named Mark Stanfield, of whom I know absolutely nothing; I feel confident that director Lindsay-Hogg had more than a bit to do with the script.) Two Of Us is not a biography of the Beatles; it has very little plot, in fact, and takes place all in one day in New York City. What it does is imagine a meeting between John and Paul in 1976, while John lived in New York. That meeting is entirely fictitious, of course \\x96 though it can't truly be disproved that such a meeting actually took place. But through that imagined conversation it gives us a glimpse into the personalities of these two great musicians \\x96 their intelligence, their sense of humor, their different reaction to stardom, and most of all their relationship; what made them such a great team, and what broke them up.Since it's a talk movie, nothing much except for dialogue between two characters for an hour and a half, it's likely to bore all but true fans of the Beatles; but it's a fantastic piece of writing and storytelling, and is both informative and touching. For those interested in these two musical giants, very quickly you'll get over the shock of how different the actors look from their counterparts and feel like John and Paul had come to life \\x96 so intimate and convincing is the script, and so committed are \",\n",
       "        True]], dtype=object)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_df[compare_df.tfidf_right].reindex(top_disagree_idx).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3570f38b-f758-458f-b1ef-b7413ef5e9c4",
   "metadata": {},
   "source": [
    "### Word embeddings\n",
    "Our next approach is to include context in the word-level representations.  We'll be bringing SpaCy into the mix here, particularly their \"medium\" English web model, which uses GloVe embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bb1334-e39e-4ee3-abd0-8c91acbbe406",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33430bf0-6c80-4eae-a807-9a50190659d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only need to run this once\n",
    "#!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abdfd44-0837-43a5-8a84-4d27bb70fe9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16b746a-3b95-4519-adfb-b48b111964fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GloveVectorizer(BaseEstimator, TransformerMixin):\n",
    "    # this is a custom document transformer for use in the scikit-learn pipeline\n",
    "    def __init__(self, vectorizer):\n",
    "        self.vectorizer = vectorizer\n",
    "        return\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.vectorizer.fit(X)\n",
    "        vocab = self.vectorizer.vocabulary_\n",
    "        self.vocab_glove = np.zeros(shape=(len(vocab), 300))\n",
    "        for token, idx in vocab.items():\n",
    "            self.vocab_glove[idx] = nlp(token).vector\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = self.vectorizer.transform(X).toarray()\n",
    "        sum_words = (X_transformed.sum(1)).reshape(-1, 1)\n",
    "        glove_vecs = (X_transformed.dot(self.vocab_glove))/sum_words\n",
    "        return glove_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170d405e-ebaf-49f9-b83c-a8794ab4abd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use binary here to handle longer reviews\n",
    "count = CountVectorizer(stop_words='english', min_df=0.01, binary=False)\n",
    "glove = GloveVectorizer(count)\n",
    "\n",
    "glove_pipeline = Pipeline(\n",
    "    steps=[(\"preprocessor\", glove),\n",
    "          ('model', LogisticRegression(max_iter=500, solver='liblinear'))]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b10c82b-b418-4f60-9f65-e0208bd3097e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "glove_pipeline.fit(X_train, y_train)\n",
    "glove_pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7503d51d-d428-4bff-91df-264379d324fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.85      0.84      0.84      7522\n",
      "         pos       0.84      0.85      0.84      7478\n",
      "\n",
      "    accuracy                           0.84     15000\n",
      "   macro avg       0.84      0.84      0.84     15000\n",
      "weighted avg       0.84      0.84      0.84     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    classification_report(y_pred=glove_pipeline.predict(X_test),\n",
    "                          y_true=y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbeb639-c5e6-4162-8cc9-42e641c8db7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacy3",
   "language": "python",
   "name": "spacy3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

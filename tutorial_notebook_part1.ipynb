{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "280c2486-cf28-4e32-9b85-3caf4296a3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "13eb78f4-b61c-4e56-a1e2-153fb0482523",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219cfc03-000a-4381-a353-2778f97b2a36",
   "metadata": {},
   "source": [
    "## Bagging to BERT: A tour of applied NLP\n",
    "### Part 1: Four flavors of bagging\n",
    "### Table of Contents\n",
    "* [Data processing](#data)\n",
    "* [Word Counts](#word)\n",
    "* [TF-IDF](#tfidf)\n",
    "* [Topic models](#topic)\n",
    "* [Word vectors](#vectors)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c7e591-7d7b-4d92-8f61-d5bb03af5dd7",
   "metadata": {},
   "source": [
    "### Data processing <a class=\"anchor\" id=\"data\"></a>\n",
    "\n",
    "Up first is some preprocessing.  You'll either need to download the [imdb review data](https://ai.stanford.edu/~amaas/data/sentiment/) and save it to this directory OR download the [processed data](https://drive.google.com/file/d/1oN_fO91IBkDHD_u6WXiUCvhhyNexQDJq/view?usp=sharing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "9b3c4ac0-5a90-49ef-b03b-378932d13c9a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # processing the original data into DataFrame\n",
    "# # here for reference, don't need to run this if you're using reviews.pkl.gz\n",
    "# source_path = Path('./aclImdb/')\n",
    "# #neg_files = source_path.glob('./*/neg/*.txt')\n",
    "# #pos_files = source_path.glob('./*/pos/*.txt')\n",
    "# all_files = []\n",
    "# for f in source_path.glob('./*/*/*.txt'):\n",
    "#     filename = f.as_posix()\n",
    "#     if 'unsup' not in filename:\n",
    "#         # split up into useful components\n",
    "#         _, split, sent, idx = filename.split('/')\n",
    "#         idx = int(idx.split('_')[0])\n",
    "#         all_files.append([idx, split, sent, f.read_text()])\n",
    "# review_df = pd.DataFrame(all_files)\n",
    "# review_df.columns = ['idx', 'split', 'label', 'text']\n",
    "# # some minor html cruft is in here\n",
    "# review_df['text'] = review_df['text'].str.replace('<br /><br />', '')\n",
    "# review_df = review_df.to_pickle('reviews.pkl.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "e28f61ea-8e62-4d21-8f5a-6e3eb1bed2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can skip here if you already have reviews.pkl.gz\n",
    "review_df = pd.read_pickle('reviews.pkl.gz')\n",
    "review_df['label'] = review_df['label'] == 'pos'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db840c0f-9f6d-4058-9913-356eb700dcd0",
   "metadata": {},
   "source": [
    "### Word counts  <a class=\"anchor\" id=\"data\"></a>\n",
    "A very basic way to use a sanitized list of tokens is to do a word count. This unlocks a lot of insights right off and is an important step in exploratory data analysis in text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "e2646a7d-b0f4-406d-bb68-7f5351b425b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n",
      " Alan Rickman & Emma Thompson give good performances with southern/New Orleans accents in this detective flick. It's worth seeing for their scenes- and Rickman's scene with Hal Holbrook. These three actors mannage to entertain us no matter what the movie, it seems. The plot for the movie shows potential, but one gets the impression in watching the film that it was not pulled off as well as it could have been. The fact that it is cluttered by a rather uninteresting subplot and mostly uninteresting kidnappers really muddles things. The movie is worth a view- if for nothing more than entertaining performances by Rickman, Thompson, and Holbrook. \n",
      "\n",
      "Positive\n",
      " Based on an actual story, John Boorman shows the struggle of an American doctor, whose husband and son were murdered and she was continually plagued with her loss. A holiday to Burma with her sister seemed like a good idea to get away from it all, but when her passport was stolen in Rangoon, she could not leave the country with her sister, and was forced to stay back until she could get I.D. papers from the American embassy. To fill in a day before she could fly out, she took a trip into the countryside with a tour guide. \"I tried finding something in those stone statues, but nothing stirred in me. I was stone myself.\" Suddenly all hell broke loose and she was caught in a political revolt. Just when it looked like she had escaped and safely boarded a train, she saw her tour guide get beaten and shot. In a split second she decided to jump from the moving train and try to rescue him, with no thought of herself. Continually her life was in danger. Here is a woman who demonstrated spontaneous, selfless charity, risking her life to save another. Patricia Arquette is beautiful, and not just to look at; she has a beautiful heart. This is an unforgettable story. \"We are taught that suffering is the one promise that life always keeps.\"\n"
     ]
    }
   ],
   "source": [
    "# take a positive and negative review for examples\n",
    "# we'll use Star Wars Episode VI since everyone likes a Star War\n",
    "neg_review = review_df.loc[~review_df.label].iloc[0]['text']\n",
    "pos_review = review_df[review_df.label].iloc[0]['text']\n",
    "print('Negative\\n', neg_review, '\\n')\n",
    "print('Positive\\n', pos_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "1ee60e2b-b1b5-4342-9e7e-8e0e3f17538f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'the': 4, 'it': 4, 'for': 3, 'and': 3, 'The': 3, 'performances': 2, 'with': 2, 'in': 2, 'worth': 2, 'Holbrook.': 2, 'movie': 2, 'that': 2, 'as': 2, 'is': 2, 'by': 2, 'a': 2, 'uninteresting': 2, 'Alan': 1, 'Rickman': 1, '&': 1, 'Emma': 1, 'Thompson': 1, 'give': 1, 'good': 1, 'southern/New': 1, 'Orleans': 1, 'accents': 1, 'this': 1, 'detective': 1, 'flick.': 1, \"It's\": 1, 'seeing': 1, 'their': 1, 'scenes-': 1, \"Rickman's\": 1, 'scene': 1, 'Hal': 1, 'These': 1, 'three': 1, 'actors': 1, 'mannage': 1, 'to': 1, 'entertain': 1, 'us': 1, 'no': 1, 'matter': 1, 'what': 1, 'movie,': 1, 'seems.': 1, 'plot': 1, 'shows': 1, 'potential,': 1, 'but': 1, 'one': 1, 'gets': 1, 'impression': 1, 'watching': 1, 'film': 1, 'was': 1, 'not': 1, 'pulled': 1, 'off': 1, 'well': 1, 'could': 1, 'have': 1, 'been.': 1, 'fact': 1, 'cluttered': 1, 'rather': 1, 'subplot': 1, 'mostly': 1, 'kidnappers': 1, 'really': 1, 'muddles': 1, 'things.': 1, 'view-': 1, 'if': 1, 'nothing': 1, 'more': 1, 'than': 1, 'entertaining': 1, 'Rickman,': 1, 'Thompson,': 1})\n"
     ]
    }
   ],
   "source": [
    "# base python word count - split on whitespace, use Counter object)\n",
    "print(Counter(neg_review.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112c7f82-7e21-42a1-9f70-0c97345ab36a",
   "metadata": {},
   "source": [
    "Already see some things that need to be considered; capitalization treats \"The\" and \"the\" differently, words like \"the\" and \"it\" dominate counts.\n",
    "\n",
    "Luckily, scikit-learn's CountVectorizer allows for simple preprocessing like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "3868f067-62e5-4227-9e81-76485ec7cfdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x76 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 76 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scikit-learn's countvectorizer\n",
    "count = CountVectorizer()\n",
    "neg_vec = count.fit_transform([neg_review])\n",
    "neg_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b990345-6215-4e55-845e-53ad8d5c5e80",
   "metadata": {},
   "source": [
    "`CountVectorizer` outputs a sparse matrix by default.  We can convert that to a normal numpy array and stitch it together with the vocabulary from the `fit()` call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "5628123c-2f0a-48b5-afa3-faf6284f60f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accents': 1, 'actors': 1, 'alan': 1, 'and': 3, 'as': 2, 'been': 1, 'but': 1, 'by': 2, 'cluttered': 1, 'could': 1, 'detective': 1, 'emma': 1, 'entertain': 1, 'entertaining': 1, 'fact': 1, 'film': 1, 'flick': 1, 'for': 3, 'gets': 1, 'give': 1, 'good': 1, 'hal': 1, 'have': 1, 'holbrook': 2, 'if': 1, 'impression': 1, 'in': 2, 'is': 2, 'it': 5, 'kidnappers': 1, 'mannage': 1, 'matter': 1, 'more': 1, 'mostly': 1, 'movie': 3, 'muddles': 1, 'new': 1, 'no': 1, 'not': 1, 'nothing': 1, 'off': 1, 'one': 1, 'orleans': 1, 'performances': 2, 'plot': 1, 'potential': 1, 'pulled': 1, 'rather': 1, 'really': 1, 'rickman': 3, 'scene': 1, 'scenes': 1, 'seeing': 1, 'seems': 1, 'shows': 1, 'southern': 1, 'subplot': 1, 'than': 1, 'that': 2, 'the': 7, 'their': 1, 'these': 1, 'things': 1, 'this': 1, 'thompson': 2, 'three': 1, 'to': 1, 'uninteresting': 2, 'us': 1, 'view': 1, 'was': 1, 'watching': 1, 'well': 1, 'what': 1, 'with': 2, 'worth': 2}\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    dict(zip(count.get_feature_names_out(), \n",
    "             neg_vec.toarray().flatten())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587c23b3-8d69-4a1b-b2e8-726f88eeb977",
   "metadata": {},
   "source": [
    "We can see the defaults have already done some amount of cleaning for us.\n",
    "\n",
    "#### Deterministic Approach with word counts\n",
    "\n",
    "Let's try a deterministic approach, using word counts and a list of \"positive\" vs \"negative\" words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "251c4e13-03bf-4110-b15a-c128c12c7e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_words = [\"good\", \"great\", \"like\", \"loved\"]\n",
    "neg_words = [\"bad\", \"awful\", \"dislike\", \"hated\"]\n",
    "\n",
    "# we're going to use this train/test split throughout\n",
    "# we'll also use this seed for consistency\n",
    "# NOTE: Usually you'll want to do a separate validation set when choosing models/featuresets!\n",
    "seed = 37\n",
    "np.random.seed(seed)\n",
    "pct_train = 0.7\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    review_df['text'],\n",
    "    review_df['label'], train_size=pct_train)\n",
    "\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "train_vecs = cv.fit_transform(X_train)\n",
    "feats = cv.get_feature_names_out()\n",
    "pos_idxs = np.where(np.isin(feats, pos_words))[0]\n",
    "neg_idxs = np.where(np.isin(feats, neg_words))[0]\n",
    "train_det_score = train_vecs[:, pos_idxs].sum(1) - train_vecs[:, neg_idxs].sum(1)\n",
    "# easier for group-level score\n",
    "train_det_score = pd.Series(np.array(train_det_score).ravel(), \n",
    "                            index=X_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "765c8284-bb84-46e1-982e-928e140eeef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our threshold - the average score for negative, that or below = negative\n",
    "neg_thresh = train_det_score.groupby(review_df['label'].loc[X_train.index]).mean()[False]\n",
    "test_vecs = cv.transform(X_test)\n",
    "test_det_score = test_vecs[:, pos_idxs].sum(1) - test_vecs[:, neg_idxs].sum(1)\n",
    "det_pred = test_det_score>neg_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "id": "b7ea3fdf-6cb4-4602-a384-dd05460ac21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.61      0.44      0.51      7522\n",
      "        True       0.56      0.71      0.63      7478\n",
      "\n",
      "    accuracy                           0.58     15000\n",
      "   macro avg       0.58      0.58      0.57     15000\n",
      "weighted avg       0.58      0.58      0.57     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    classification_report(y_pred=det_pred,\n",
    "                          y_true=y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1f69c5-72b7-4ae9-8ed8-969818832c07",
   "metadata": {},
   "source": [
    "#### Count Vector + Logistic Regression \n",
    "Here we try a count vector with Logistic Regression.  This alleviates the need for chosing an arbitrary set of terms and arbitrary threshold as above.\n",
    "\n",
    "Here I use scikit-learn's [Pipelines](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) functionality.  I won't try and explain that here, the docs do a much better job than I can.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "25a9d696-97ba-415d-84d2-503844edf3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = CountVectorizer(stop_words='english')\n",
    "\n",
    "count_pipeline = Pipeline(\n",
    "    steps=[(\"preprocessor\", count),\n",
    "          ('model', LogisticRegression(max_iter=500, solver='liblinear'))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "952acf7e-5a15-4bb8-88d1-258a634e5cce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8813333333333333"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "count_pipeline.fit(X_train, y_train)\n",
    "count_pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "f58d9f7f-53b2-4cbb-92e6-2d5bac6dcf9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.89      0.88      0.88      7522\n",
      "        True       0.88      0.89      0.88      7478\n",
      "\n",
      "    accuracy                           0.88     15000\n",
      "   macro avg       0.88      0.88      0.88     15000\n",
      "weighted avg       0.88      0.88      0.88     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    classification_report(y_pred=count_pipeline.predict(X_test),\n",
    "                          y_true=y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30256a1-982a-4aee-877c-7a7045580aa2",
   "metadata": {},
   "source": [
    "This is actually really good! 88% of the time we're predicting the right class with this model.  But can we do...better?\n",
    "\n",
    "### TF-IDF <a class=\"anchor\" id=\"tfidf\"></a>\n",
    "One thing we notice with count vectors is that all words are being counted the same.  We might want to use a weighting scheme to ensure that words that are more informative about the content are flagged as more important.  One weighting scheme is Term Frequency - Inverse Document Frequency (TF-IDF).\n",
    "\n",
    "Take as an example some kind of simplistic movie reviews.  We can already tell which words are most relevant to the specific content of each review (i.e. \"good\", \"bad\", \"great\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "385e0955-d159-4d27-8508-c1a5645a6dcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bad</th>\n",
       "      <th>good</th>\n",
       "      <th>great</th>\n",
       "      <th>movie</th>\n",
       "      <th>the</th>\n",
       "      <th>was</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bad  good  great  movie  the  was\n",
       "0    0     1      0      1    1    1\n",
       "1    1     0      0      1    1    1\n",
       "2    0     0      1      1    1    1"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = ['The movie was good',\n",
    "        'The movie was bad',\n",
    "        'The movie was great']\n",
    "\n",
    "cv = CountVectorizer()\n",
    "vecs = cv.fit_transform(docs).toarray()\n",
    "# we'll use pandas DF for easier display\n",
    "pd.DataFrame(vecs, columns=cv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9d93d3-aa56-4599-bd57-bd4e12232445",
   "metadata": {},
   "source": [
    "You'll notice that `vecs` contains the term frequencies.  If we use sklearn's `TfidfVectorizer`, it will calculate those term counts and then multiply them by the Inverse Document Frequency (IDF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "30f3611e-8860-411c-b297-e9d2eedca714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bad</th>\n",
       "      <th>good</th>\n",
       "      <th>great</th>\n",
       "      <th>movie</th>\n",
       "      <th>the</th>\n",
       "      <th>was</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.69903</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.412859</td>\n",
       "      <td>0.412859</td>\n",
       "      <td>0.412859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.69903</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.412859</td>\n",
       "      <td>0.412859</td>\n",
       "      <td>0.412859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.69903</td>\n",
       "      <td>0.412859</td>\n",
       "      <td>0.412859</td>\n",
       "      <td>0.412859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       bad     good    great     movie       the       was\n",
       "0  0.00000  0.69903  0.00000  0.412859  0.412859  0.412859\n",
       "1  0.69903  0.00000  0.00000  0.412859  0.412859  0.412859\n",
       "2  0.00000  0.00000  0.69903  0.412859  0.412859  0.412859"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "# we'll use pandas DF for easier display\n",
    "tfidf_vecs = tfidf.fit_transform(docs).toarray()\n",
    "tfidf_df = pd.DataFrame(tfidf_vecs, columns=tfidf.get_feature_names_out())\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a464164b-5882-48d7-8bb9-f48008ca8d92",
   "metadata": {},
   "source": [
    "You can see that the discriminative words have higher weight than the non-discriminative words.  \n",
    "\n",
    "It's worth noting here - in terms of \"separability\", having 0 v 1 (count of \"good\" vs count of \"bad\") might actually be better.  But these are highly curated examples - you can imagine cases where good and bad descriptive terms are mixed in a review, you want to capture the words that describe better the \"aboutness\" of the review.  (Think: \"This movie was not bad, it was good!\")\n",
    "\n",
    "Now let's fit our regression as above with TF-IDF vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "28f28bdf-371d-4dd7-a25b-63376f3f76d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use binary here to handle longer reviews\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "tfidf_pipeline = Pipeline(\n",
    "    steps=[(\"preprocessor\", tfidf),\n",
    "          ('model', LogisticRegression(max_iter=500, solver='liblinear'))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "0c7f2b41-18ed-4fab-adee-d5b7bd44c90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8912666666666667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.90      0.88      0.89      7522\n",
      "        True       0.88      0.90      0.89      7478\n",
      "\n",
      "    accuracy                           0.89     15000\n",
      "   macro avg       0.89      0.89      0.89     15000\n",
      "weighted avg       0.89      0.89      0.89     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "tfidf_pipeline.fit(X_train, y_train)\n",
    "print(f'accuracy: {tfidf_pipeline.score(X_test, y_test)}')\n",
    "print(\n",
    "    classification_report(y_pred=tfidf_pipeline.predict(X_test),\n",
    "                          y_true=y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "10b5b18b-9e54-4ba9-afd3-9a63318aa93e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>tfidf</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6408</th>\n",
       "      <td>awful</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6697</th>\n",
       "      <td>bad</td>\n",
       "      <td>2.0</td>\n",
       "      <td>316.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10324</th>\n",
       "      <td>boring</td>\n",
       "      <td>5.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22357</th>\n",
       "      <td>disappointing</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22360</th>\n",
       "      <td>disappointment</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27335</th>\n",
       "      <td>excellent</td>\n",
       "      <td>89233.0</td>\n",
       "      <td>89227.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28106</th>\n",
       "      <td>fails</td>\n",
       "      <td>17.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34078</th>\n",
       "      <td>great</td>\n",
       "      <td>89234.0</td>\n",
       "      <td>89112.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50143</th>\n",
       "      <td>mediocre</td>\n",
       "      <td>30.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51096</th>\n",
       "      <td>mildly</td>\n",
       "      <td>96.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52920</th>\n",
       "      <td>mst3k</td>\n",
       "      <td>59.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58910</th>\n",
       "      <td>perfect</td>\n",
       "      <td>89232.0</td>\n",
       "      <td>89234.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60835</th>\n",
       "      <td>poor</td>\n",
       "      <td>6.0</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78768</th>\n",
       "      <td>terrible</td>\n",
       "      <td>7.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86068</th>\n",
       "      <td>waste</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86083</th>\n",
       "      <td>wasting</td>\n",
       "      <td>100.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87842</th>\n",
       "      <td>worst</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 word    tfidf    count\n",
       "6408            awful      3.0      4.0\n",
       "6697              bad      2.0    316.0\n",
       "10324          boring      5.0     14.0\n",
       "22357   disappointing     16.0      5.0\n",
       "22360  disappointment     14.0      2.0\n",
       "27335       excellent  89233.0  89227.0\n",
       "28106           fails     17.0      9.0\n",
       "34078           great  89234.0  89112.0\n",
       "50143        mediocre     30.0      6.0\n",
       "51096          mildly     96.0     10.0\n",
       "52920           mst3k     59.0      7.0\n",
       "58910         perfect  89232.0  89234.0\n",
       "60835            poor      6.0     36.0\n",
       "78768        terrible      7.0     20.0\n",
       "86068           waste      4.0      1.0\n",
       "86083         wasting    100.0      8.0\n",
       "87842           worst      1.0      3.0"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking at the coefficients on the LR for each model\n",
    "word_feats = tfidf_pipeline['preprocessor'].get_feature_names_out()\n",
    "# get the largest by magnitude, stitch together to compare\n",
    "top = 10\n",
    "top_tfidf = np.argsort(np.abs(tfidf_pipeline['model'].coef_.flatten()))[-top:]\n",
    "top_count = np.argsort(np.abs(count_pipeline['model'].coef_.flatten()))[-top:]\n",
    "# top\n",
    "coef_df = pd.DataFrame([\n",
    "    word_feats,\n",
    "    tfidf_pipeline['model'].coef_.flatten(),\n",
    "    count_pipeline['model'].coef_.flatten()],\n",
    "    index=['word', 'tfidf', 'count']).T\n",
    "# normalize result for compare\n",
    "coef_df['tfidf'] = coef_df['tfidf'].rank()\n",
    "coef_df['count'] = coef_df['count'].rank()\n",
    "coef_df.loc[np.unique(np.concatenate([top_tfidf, top_count]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "a3e0d90b-ec5f-46ff-a3da-6684ad26778f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examples where there's disagreement\n",
    "tfidf_pred = tfidf_pipeline.predict_proba(X_test)[:, 1]\n",
    "count_pred = count_pipeline.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "c456f39f-b53c-4293-aadf-a4725db61123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# most interesting are where there's the largest disagreement\n",
    "top_disagree_idx = np.argsort(np.abs(tfidf_pred - count_pred))[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "895f8ac0-64a1-43c4-aeac-ba6488050b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assemble in df\n",
    "compare_df = pd.DataFrame([tfidf_pred, count_pred, y_test, X_test],\n",
    "            index=['tfidf_pred', 'count_pred', 'label', 'text']).T\n",
    "# would like some shorter mv reviews here\n",
    "compare_df['text'] = compare_df['text'].apply(lambda x: x[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "b91b04da-8631-4ce1-ae8c-3a6796572fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_df['tfidf_right'] = ((compare_df['tfidf_pred']>=0.5)&(compare_df['label']=='pos'))|\\\n",
    "    ((compare_df['tfidf_pred']<0.5)&(compare_df['label']=='neg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "7adecafb-b87b-4382-a781-c90d9d0c5e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple way to look at some of these differences\n",
    "#compare_df[compare_df.tfidf_right].reindex(top_disagree_idx).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8482b4e9-8a10-461a-93bb-cc1806ea8879",
   "metadata": {},
   "source": [
    "It's difficult to see piecemeal, but it does appear that certain words we associate with negative reviews (e.g. \"bad\") have a stronger influence on prediction in the TF-IDF model.\n",
    "\n",
    "### Topic Models <a class=\"anchor\" id=\"topic\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "1c5675d7-407d-4127-9858-42c0faaa7d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_components(model, word_features, top_display=5):\n",
    "    # utility for displaying respresentative words per component for topic models\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        top_words_idx = topic.argsort()[::-1][:top_display]\n",
    "        top_words = [word_features[i] for i in top_words_idx]\n",
    "        print(\" \".join(top_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "id": "b68b7854-4689-4688-b8b2-bf6aca0de643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the number of components (topics)\n",
    "n_components = 10\n",
    "# adding a few tweaks, just based on experimentation\n",
    "nmf = NMF(n_components=n_components,\n",
    "          init='nndsvda',\n",
    "         max_iter=500)\n",
    "# NMF typically uses tfidf, not word counts\n",
    "# fit tfidf vectorizer\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf_vecs = tfidf.fit_transform(review_df['text'])\n",
    "nmf_vecs = nmf.fit_transform(tfidf_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b799b42-2e02-4d29-a2d1-cae86544135d",
   "metadata": {},
   "source": [
    "Both NMF provides a components matrix which corresponds to the loading of each word on each topic.  Higher values means the word is more relevant to that topic.  With the function below, we can display some of the \"representative\" words from each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "id": "fa3bcf3b-c3e9-4df8-b21c-e564c85addca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "life man story character young\n",
      "Topic 1:\n",
      "movie movies seen watch saw\n",
      "Topic 2:\n",
      "bad acting worst terrible good\n",
      "Topic 3:\n",
      "film films seen director saw\n",
      "Topic 4:\n",
      "just like don really people\n",
      "Topic 5:\n",
      "great good story really best\n",
      "Topic 6:\n",
      "series episode tv episodes season\n",
      "Topic 7:\n",
      "horror gore budget effects scary\n",
      "Topic 8:\n",
      "book read novel story version\n",
      "Topic 9:\n",
      "funny comedy jokes laugh humor\n"
     ]
    }
   ],
   "source": [
    "display_components(nmf, tfidf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29842060-d538-4d93-b643-3d8aaf174518",
   "metadata": {},
   "source": [
    "One neat thing here is we can see which reviews load highly, for example, on this \"horror\" topic (topic 7)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "573c0925-49b6-4baa-a666-d0b5c21ee2d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"The film 'Nightbreed' is one of the best horror films I have ever seen. Overall, I'm not a big fan of horror films, but there is something about this film that is more atmospheric and different from any other horror film I have ever seen. Many horror films i've seen i've enjoyed watching, however, as they are based on horror, I know that the stories are unreal, as they are fictional, therefore I can't take them all seriously. Nightbeed, on the other hand, is a unique horror Genre as it has a feel of realism that i've seen in very few other horror films.This films story on how a man gets murdered and ends up living with the undead in an underground cemetery shelter with undead monsters is the kind of story a person would get from a dreaming Nightmare as its a very unique and original storyline. Most horror films i've seen are all quite fake, but because Nightbreed was so incredibly sophisticated and geniously directed with superb acting, especially by Craig sheffer (Aaron Boone) amazing special effects, great lighting and fantastic dialogue, I found this film to have a sense of depth and maturity with no silly fake horror parody, whatsoever, that many other horror films have. Nightbreed, as well as being horror has elements of thriller, romance and action all rapped in one. If you haven't seen this film, I recommend you watch it, as I rate it a 10/10.\",\n",
       "       \"This is one of the worst horror movies I have ever seen... Unfortunately, I am a horror movie buff and will rent any horror movie unless it's not made for t.v. When looking at the box it says it is rated R for gore and some language... Where was the gore? Was their one good death scene where you actually saw gore? I could have overlooked that if there had been some brief nudity or some good dialogue. There wasn't even one remotely witty or amusing line in this lame movie. Sometimes horror movies are awesome because they are so stupid, but this was just sad.\"],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df.iloc[np.argsort(nmf_vecs[:, 7])[-2:]]['text'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b26740-a871-4c73-a42a-18a9835f5149",
   "metadata": {},
   "source": [
    "Will you look at that - we might have a nice way of \"categorizing\" here.  Let's see how this does with sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "f963e47c-6508-45c7-8ff7-2c3742fee335",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "nmf = NMF(n_components=n_components,\n",
    "          init='nndsvda',\n",
    "          max_iter=500)\n",
    "clf_nmf_pipeline = Pipeline(\n",
    "    steps=[(\"preprocessor\", tfidf),\n",
    "           (\"topic\", nmf),\n",
    "          ('model', LogisticRegression(max_iter=500, solver='liblinear'))]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "a02f3288-f8b7-4bd4-ae62-0bac6cb4bdf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7588666666666667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.80      0.70      0.74      7522\n",
      "        True       0.73      0.82      0.77      7478\n",
      "\n",
      "    accuracy                           0.76     15000\n",
      "   macro avg       0.76      0.76      0.76     15000\n",
      "weighted avg       0.76      0.76      0.76     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "clf_nmf_pipeline.fit(X_train, y_train)\n",
    "print(f'accuracy: {clf_nmf_pipeline.score(X_test, y_test)}')\n",
    "print(\n",
    "    classification_report(y_pred=clf_nmf_pipeline.predict(X_test),\n",
    "                          y_true=y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c03126-fd7a-48f3-ad36-ddd074fc4836",
   "metadata": {},
   "source": [
    "It's not great, but we have much fewer features.  With some tuning, we might be able to get compareable performance with a much smaller feature vector.\n",
    "\n",
    "We may also want to combine these topic vectors with a smaller set of word counts.  Again, this requires some tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "fe9983bb-8324-487b-aadd-d7da3c6c5637",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_small = TfidfVectorizer(min_df=0.05, stop_words='english')\n",
    "\n",
    "clf_union_pipeline = Pipeline(\n",
    "    steps=[('feats',\n",
    "            FeatureUnion(\n",
    "                [(\"tfdf\", tfidf_small), \n",
    "                 (\"nmf\", nmf_pipeline)])),\n",
    "          ('model', LogisticRegression(max_iter=500, solver='liblinear'))]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "9f28ba72-8aff-4d6c-bfa4-da19a7546a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8064666666666667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.82      0.79      0.80      7522\n",
      "        True       0.79      0.83      0.81      7478\n",
      "\n",
      "    accuracy                           0.81     15000\n",
      "   macro avg       0.81      0.81      0.81     15000\n",
      "weighted avg       0.81      0.81      0.81     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "clf_union_pipeline.fit(X_train, y_train)\n",
    "print(f'accuracy: {clf_union_pipeline.score(X_test, y_test)}')\n",
    "print(\n",
    "    classification_report(y_pred=clf_union_pipeline.predict(X_test),\n",
    "                          y_true=y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3570f38b-f758-458f-b1ef-b7413ef5e9c4",
   "metadata": {},
   "source": [
    "### Word vectors <a class=\"anchor\" id=\"vectors\"></a>\n",
    "Our next approach is to include context in the word-level representations.  We'll be bringing SpaCy into the mix here, particularly their \"medium\" English web model, which uses GloVe embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "a7bb1334-e39e-4ee3-abd0-8c91acbbe406",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "33430bf0-6c80-4eae-a807-9a50190659d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only need to run this once\n",
    "#!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "2abdfd44-0837-43a5-8a84-4d27bb70fe9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "b16b746a-3b95-4519-adfb-b48b111964fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GloveVectorizer(BaseEstimator, TransformerMixin):\n",
    "    # this is a custom document transformer for use in the scikit-learn pipeline\n",
    "    def __init__(self, vectorizer):\n",
    "        self.vectorizer = vectorizer\n",
    "        return\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # extracts GloVe vector for all words in vectorizer vocabulary (WxV)\n",
    "        self.vectorizer.fit(X)\n",
    "        vocab = self.vectorizer.vocabulary_\n",
    "        self.vocab_glove = np.zeros(shape=(len(vocab), 300))\n",
    "        for token, idx in vocab.items():\n",
    "            self.vocab_glove[idx] = nlp(token).vector\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        # create the average GloVe vector for all words in document (D)\n",
    "        # use vectorizer to create DxW vector\n",
    "        X_transformed = self.vectorizer.transform(X).toarray()\n",
    "        # sum of words in D\n",
    "        sum_words = (X_transformed.sum(1)).reshape(-1, 1)\n",
    "        # create DxV vectors\n",
    "        glove_vecs = (X_transformed.dot(self.vocab_glove))/sum_words\n",
    "        return glove_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "451c76ee-03de-4eb1-a970-a78a45eea54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use binary here to handle longer reviews\n",
    "count = CountVectorizer(stop_words='english', min_df=0.01, binary=False)\n",
    "glove = GloveVectorizer(count)\n",
    "\n",
    "glove_pipeline = Pipeline(\n",
    "    steps=[(\"preprocessor\", glove),\n",
    "          ('model', LogisticRegression(max_iter=500, solver='liblinear'))]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "id": "1b10c82b-b418-4f60-9f65-e0208bd3097e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8426"
      ]
     },
     "execution_count": 512,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "glove_pipeline.fit(X_train, y_train)\n",
    "glove_pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "7503d51d-d428-4bff-91df-264379d324fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.85      0.84      0.84      7522\n",
      "        True       0.84      0.85      0.84      7478\n",
      "\n",
      "    accuracy                           0.84     15000\n",
      "   macro avg       0.84      0.84      0.84     15000\n",
      "weighted avg       0.84      0.84      0.84     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    classification_report(y_pred=glove_pipeline.predict(X_test),\n",
    "                          y_true=y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93659c0c-ac2c-468f-8098-7b8e227f08c6",
   "metadata": {},
   "source": [
    "In part 2 of the tutorial, we'll be digging into more complex models and breaking out of the \"bagging\" paradigm.  I've separated that portion, as it will likely require use of a GPU to be effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe82bfb-2020-4b58-b70e-7eff6cdec0cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacy3",
   "language": "python",
   "name": "spacy3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
